# 11 采样方法
大多数概率模型解析解都很难求得，所以需要近似，第十章为确定近似，主要方法为贝叶斯变分法和期望传播。本章的推论方法为数值近似，或者叫蒙特卡洛技术。虽然很多时候我们的都对计算后验概率感兴趣，但是实际上很多时候我们的最终目标实际上是根据后验概率计算某个函数的期望比如预测一个新数据点。此时我们的问题为计算p(z)概率下f(z)函数的期望。此时，我们的主要目标是根据p(z)概率进行采样，计算f值最后取平均。

$$\hat f = \frac 1 L \sum_{l=1} ^ L f( z ^{(l)}))$$

这样采样得到的期望 等于原函数期望。 这样得到期望的精度和Z的维度无关。一般来说使用10-20个采样就可以得到相对较高的精度了。不过采样经常不满足IDD 条件，这样使得有效样本量比看起来的要小。还有如果f(z)在p(z)很的地方有很大的值，则可能要增加采样数量提高精度。

对于某些值已经观测到的有向图一个简单方法是先采样，然后看是否观测值是否符合。不符合的丢弃。这个方法的缺点是效率太低。而对于无向图则不存在类似的单向采样方法，必须采样更加计算复杂型的方法，比如吉布斯采样。如果要求边缘概率的采样，则简单进行条件概率采样然后忽视边缘项即可。

## 11.1 基本采样算法
计算机默认可以生成（0, 1）之间的伪随机数。
### 11.1.1 标准分布
需要将（0，1）伪随机数z根据某种分布映射到变量y上面去。映射为$y = f(z)$，而y的取值又要符合概率分布$p(y)$。可以得到

$$z= h(y) = \int_{- \inf}^y p(\hat y ) d \hat y$$

而变换取$y = h^{-1}(z)$ 即可。
对于多变量，此方法需要用雅克比行列式。

**Box-Muller**方法：先进行正方形采样，然后去掉圆外的点得到圆形采样。再做变换得到高斯采样。

总结这种转换方法通过反转不定积分获得所需分布。但是只适合少数简单分布情况。这里引入两种技术：**rejection sampling（拒绝采样）** 和 **importance sampling（重要采样）**虽然他们只适合单变量情况，但是确实更普适方法的重要组成部分。

### 11.1.2 拒绝采样

前提：函数很复杂没法用标准采样，我们知道任意位置的p(z)。

$$p(z) =  \frac 1 {Z_p} \hat p (z)$$

其中$\hat p(z)$已知但是$Z_p$不知道。

先按照$q(z)$采样，然后如果$kq(z) > \hat p(z)$ 则抛弃这个点，否则留下。可以计算出

$$p(采样) = \int \frac{\hat p(z) }{kq(z)}q(z)dz$$

### 11.1.3 自适应拒绝采样

可以用分段函数直线将目标函数围起来，然后再拒绝采样，这样可以避免复杂的计算。拒绝采样受维度灾变影响很大，一个维度损失1%，1000个维度则可能只有1/20000的采样可以被接受。所以无法运用在高维情况。

### 11.1.4 重要采样
采样的一个很重要的作用是计算公式11.1中的期望。重要采样可以直接计算期望，而不需要按照$p(z)$进行采样。如果我们无法直接从$p(z)$采样但是可以很容易计算p(z)则我们可以网格化$p(z)$计算每一个网格中的$p(z^l)f(z^l)$。但是这样随着维度增加效率低下。

重要采样是从$q(z)$采样，计算$f(z) \frac{p(z)}{q(z)}$其中$r_l = \frac{p(z^l)}{q(z^l)}$为重要权重。

### 11.1.5 采样-重要-重采样
可以避免决定需要采样的样本数k。（未看明白）

### 11.1.6 采样和EM算法
采样可以用来求EM算法中E步骤。特别是当E步骤没有解析解时。可以将其中的期望改为采样。这样可以将M步骤需要计算的最大值简化为

$$Q(\theta, \theta^{old}) = \frac {1} {L} \sum_{l=1}^L \ln p (Z ^(l), X | \theta)$$

这个方法叫蒙特卡洛EM算法。其特例为随机EM，每次E步骤更新一个采样。其完整贝叶斯处理，引入先验概率，这时候就是IP算法。

## 11.2 马尔科夫链蒙特卡洛

可以在高维表现良好。具体采样方法为
![85fa034e90f52d4d0bf3f6fd92cdb402.png](evernotecid://60614192-C7C5-4611-9CAC-5D15AB1BE3D8/appyinxiangcom/7169492/ENNote/p1407?hash=85fa034e90f52d4d0bf3f6fd92cdb402)
这个算法叫Metropolis-Hastings算法。

### 11.2.1 马尔科夫链
基本思路是：

$$p(z^  (m+1) | z ^(1), z^(2)...z^(m)) = p(z^(m+1) | z^(m))$$


如果马尔科夫链的转移概率不随m变化，则成为homogeneous(同质)的。


### 11.2.2 Metropolis-Hastings algorithm

### 11.3 吉布斯采样

是一种广泛使用的MCMC算法也是上一章中的MH算法的一个特例。在吉布斯采用中我们一次替换分布中变量。

```
1. Initialation{z, i=1,...,M}
2. for t = 1, ... T:
    for i = 1, ... M:
        sample z(it)
```

吉布斯采样必须满
    1. 随步数不变。
    2. 
    
    
### 11.4 切片采样
基于随机游走的`Metropolis`算法有个大问题在于对步长敏感，如果步长太长则拒绝率太高，如果步长太低则游走到边缘需要的步数很高。切片采样可以提供自适应步长，自动匹配当前分布。不过它也需要我们可以计算未归一化的分布$\hatp(z)$

### 11.5 混合蒙特卡洛算法

### 估计partition 函数
