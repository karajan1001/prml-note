# Chapter 12 Continuous Latent Variables

本章主要介绍连续变隐变量的概率模型。本章的一个重要动机是很多模型中的数据都存在于一个低位的子空间中。我们可以从生成式考虑该模型，首先依据隐变量在流行上生成一个数据点，然后将观测噪声加到上面变成一个观测点。最简单的连续隐变量模型，假设观测变量和噪音都是线性高斯，这种方法就是 PCA。

## 12.1 主成分分析(Principal Component Analysis)
这是一种广泛采用的降维，有损压缩，特征提取，特征可视化方法。有两种常用定义都能得到相同的 PCA 算法。

### 12.1.1 最大方差公式
将一个D维数据集$x_n$投影到 M 维空间，使其方差最大。一维情况下，数据集的平均值为:

$$\hat{x} = \frac 1 N \sum_{n=1}^{N} x_n $$

所以方差为

$$ u^T S u $$

$$ S=\frac 1N \sum_{n=1}^N (x_n - \hat x ) (x_n - \hat x ) ^ T$$

加上约束利用拉格朗日乘子可以推得：
> 当 u 为 S 的本征向量时方差最大，最大方差为本征值。这个本征向量又叫`第一主成分`

对于其他M-1维的向量可以依次选取和之前所有本征向量正交，且本征值最大的本征向量。算法的复杂度和本征值分解一样为 $O(D^3)$。

### 12.1.2 最小误差公式

数据集可以表示为

$$\hat {x\_n}  =\sum\_{i=1}^M z\_{ni} u\_i + \sum_{i=M+1} ^ D b_i u _i$$

其中 $z\_{ni}$ 为每个不同数据点不同，而 $b\_i$ 为所有点共享同一数据

我们的目标是使得 $$J = \frac 1 N \sum_{n=1}^N || x_n - \hat{x_n}||^2 $$

最小。

可以推得J最小时$z\_{nj}$与 原始数据相同。而$b\_i$等于原始数据的平均值。和最大方差方法相似，可以推得，J的最小值在$u\_i$为`S`本征值时达到，为了让 J 更小，我们需要让`pca`子平面通过所有数据的算数平均值并且和方差最大的M个本征向量平行。

### 12.1.3 PCA的应用
两个例子:
1. 图像压缩。
2. 数据预处理。
3. 数据可视化。

在预处理中，PCA可以代替标准化。只要经过如下变化即可。

$$ y_n = L^{-\frac 1 2} U^T(x_n -\hat x)$$

**标准化(standardizing)**:

$$\rho_{ij} = \frac 1 N \sum_{n=1} ^ N \frac{(x_{ni} - \hat{x_i})(x_nj - \hat{x_j})}{\sigma_i \sigma_j}$$

其中$\sigma$为方差

### 12.1.4 PCA for high-dimensional data
如果数据的维度大多于数据量，则降维的维度上限为数据量减去1。而且因为原始空间的高纬度，导致算法复杂度过高，计算变得不可能。可以采用如下方法解决。

将N个D维空间的向量，将方差矩阵S的维度和数据点做变化。使得算法复杂度从$O(D^3)$下降为$O(N^3)$。

## 12.2 概率PCA
本节从概率隐变量模型的角度用最大似然表示PCA。概率PCA表示方法有好处：
- 概率PCA表示了一个参数数量收到限制但是依然能捕捉到数据集中领域关系的高斯分布中。
- 可以用EM算法只求解少数几个本征值，而避免立方复杂度的本征值分解。
- 概率模型和EM算法帮助对付缺失值。
- 混合概率PCA模型可以用EM方法训练得到。
- 概率PCA是贝叶斯PCA的基础。
- 似然方程可以帮助我们直接比较不同概率模型。
- 概率PCA可以用于有类别的密度分布，然后用于解决分类问题。
- 概率PCA可以用来生成数据。
