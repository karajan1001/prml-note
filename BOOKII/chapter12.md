# Chapter 12 Continuous Latent Variables

本章主要介绍连续变隐变量的概率模型。本章的一个重要动机是很多模型中的数据都存在于一个低位的子空间中。我们可以从生成式考虑该模型，首先依据隐变量在流行上生成一个数据点，然后将观测噪声加到上面变成一个观测点。最简单的连续隐变量模型，假设观测变量和噪音都是线性高斯，这种方法就是 PCA。

## 12.1 主成分分析(Principal Component Analysis)
这是一种广泛采用的降维，有损压缩，特征提取，特征可视化方法。有两种常用定义都能得到相同的 PCA 算法。

### 12.1.1 最大方差公式
将一个D维数据集$x_n$投影到 M 维空间，使其方差最大。一维情况下，数据集的平均值为:

$$\hat{x} = \frac 1 N \sum_{n=1}^{N} x_n $$

所以方差为

$$ u^T S u $$

$$ S=\frac 1N \sum_{n=1}^N (x_n - \hat x ) (x_n - \hat x ) ^ T$$

加上约束利用拉格朗日乘子可以推得：
> 当 u 为 S 的本征向量时方差最大，最大方差为本征值。这个本征向量又叫`第一主成分`

对于其他M-1维的向量可以依次选取和之前所有本征向量正交，且本征值最大的本征向量。算法的复杂度和本征值分解一样为 $O(D^3)$。

### 12.1.2 最小误差公式

数据集可以表示为

$$\hat {x\_n}  =\sum\_{i=1}^M z\_{ni} u\_i + \sum_{i=M+1} ^ D b_i u _i$$

其中 $z\_{ni}$ 为每个不同数据点不同，而 $b\_i$ 为所有点共享同一数据

我们的目标是使得 $$J = \frac 1 N \sum_{n=1}^N || x_n - \hat{x_n}||^2 $$

最小。

可以推得J最小时$z\_{nj}$与 原始数据相同。而$b\_i$等于原始数据的平均值。和最大方差方法相似，可以推得，J的最小值在$u\_i$为`S`本征值时达到，为了让 J 更小，我们需要让`pca`子平面通过所有数据的算数平均值并且和方差最大的M个本征向量平行。

### 12.1.3 PCA的应用
两个例子:
1. 图像压缩。
2. 数据预处理。
3. 数据可视化。

在预处理中，PCA可以代替标准化。只要经过如下变化即可。

$$ y_n = L^{-\frac 1 2} U^T(x_n -\hat x)$$

**标准化(standardizing)**:

$$\rho_{ij} = \frac 1 N \sum_{n=1} ^ N \frac{(x_{ni} - \hat{x_i})(x_nj - \hat{x_j})}{\sigma_i \sigma_j}$$

其中$\sigma$为方差

### 12.1.4 PCA for high-dimensional data
如果数据的维度大多于数据量，则降维的维度上限为数据量减去1。而且因为原始空间的高纬度，导致算法复杂度过高，计算变得不可能。可以采用如下方法解决。

将N个D维空间的向量，将方差矩阵S的维度和数据点做变化。使得算法复杂度从$O(D^3)$下降为$O(N^3)$。

## 12.2 概率PCA
本节从概率隐变量模型的角度用最大似然表示PCA。概率PCA表示方法有好处：
- 概率PCA表示了一个参数数量收到限制但是依然能捕捉到数据集中领域关系的高斯分布中。
- 可以用EM算法只求解少数几个本征值，而避免立方复杂度的本征值分解。
- 概率模型和EM算法帮助对付缺失值。
- 混合概率PCA模型可以用EM方法训练得到。
- 概率PCA是贝叶斯PCA的基础。
- 似然方程可以帮助我们直接比较不同概率模型。
- 概率PCA可以用于有类别的密度分布，然后用于解决分类问题。
- 概率PCA可以用来生成数据。

概率PCA模型是一个线性高斯模型的特例。其条件概率，边缘概率都是高斯。
我们假设：
$$ p(z) = N(z| 0 , I)$$
$$ p(x|z) = N(x|Wz + \mu, \sigma^2 I)$$

可以得到$x$的边缘概率分布

$$(x) = \int p(x|z)p(z)dz = N(x | \mu, C) $$ 其中 $$ C=WW^T + \sigma ^2 I$$

可以通过$M = W^TW + \sigma^2I$求得C的逆，和条件概率$$p(z|x)  = N(z|M^{-1}W^T(x-\mu), \sigma^2M)$$

### 12.2.1 最大似然PCA

用最大似然来决定刚才的概率PCA中的参数。
$$ \ln (X|W,\mu,\sigma^2) = - \frac {DN} 2 \ln(2\pi) - \frac N 2 \ln|C| -\frac 1 2 \sum_{n=1}^{N} (x_n -\mu)^TC^{-1}(x_n-\mu) $$
取导数为0可以得到$\mu = \hat x$，这是唯一极大值。而$W$和$\sigma^2$的极大值可以用同样方法求得。
$$W_{ML} = U_M(L_M-\sigma^2I)^{\frac 1 2}R$$
其中U为$D \times M$ 维矩阵其列为$S$矩阵本征向量的子集。而且当选取的本征向量的本征值为前M大时，其似然方程有最大值。 此时有:
$$\sigma_{ML}^2 = \frac 1 {D-M}\sum_{i=M+1}^D \lambda_i $$

R为任意正交变换矩阵，如果用求本征值的方法求W，则可以简单设置R为单位矩阵I。如果用EM之类的数值方法求解，则R的值不可控。当然EM方法也可以改变下，然后获得依次下降的正交主成分。

现在考虑数据点在主成分平面的映射。

$$ E[z|x] = M^{-1} W\_{ML}^T(x-\hat x)$$

其在原始空间中的点为$$W E[z|x] + \mu$$，如果取 $\sigma -> 0 $ 则可以得到对主成分平面的投影。

### 12.2.2 PCA 中的EM算法。

虽然PCA有解析解，但是使用EM算法处理依然可以获得如下好处：
1. 计算复杂度低。
2. 可以在衍生到其他没有解析解的模型。
3. 可以处理缺省值。

- E步骤:
$$E[z_n] = M^{-1}W^T(x_n - \hat x)$$
$$E[z_nz_n^T] = \sigma^2 M^{-1} + E[z_n]E[z_n]^2$$
- M步骤:
$$W_{new} = [\sum_{n=1}^N(x_n-\hat x) E[z_n]^T] [\sum_{n=1}^NE[z_nz_n^T]]^{-1}$$
$$\sigma_{new}^2 = \frac 1 {ND} \sum_{n=1} ^N ( ||x_n-\hat x|| ^ 2 - 2 * E[z_n] ^ T W_{new} ^ T (x_n - \hat x) + Tr(E[z_n z_n^T] W_{new}^T W_{new})$$

传统PCA需要的计算复杂度为$O(D^3)$ , 就我们只考虑M个主成分也需要$O(MD^2)$ 。而EM算法的计算复杂度为$O(NDM)$, 而且EM算法可以作为在线算法,每次对增量新数据进行更新。

### 12.2.3 贝叶斯 PCA

对参数$\mu, W, \sigma^2$边缘化合适的先验概率，取W的先验概率为：
$$p(W|\alpha) = \int_{i=1} ^ M (\frac{\alpha_i} {2\pi})^\frac D 2 exp ( - \frac 1 2 \alpha_i w_i^T w_i)$$
在最大边缘概率的过程中有些$\alpha$会趋于无穷,其对应的W会趋近于0。可以选取不为无穷的$\alpha$的数字为主成分的有效维度数。

### 12.2.4 因子分析

因子分析和PCA非常接近。它和概率PCA唯一的区别在于用对角方差代替各向同性方差。

$$ p(x|z) = N(x|Wz + \mu, \Phi)$$

因子分析没有解析解，其解必须由迭代求得，迭代方程为

- E步骤：
$$E[z_n] = GW^T\phi^{-1}(x_n-\hat x)$$
$$E[z_nz_n^T] = G+ E[z_n]E[z_n]^T$$
$$G=(I + W^T\phi^{-1}W)^{-1}$$

- M步骤


$$W_{new} = [\sum_{n=1}^N(x_n-\hat x) E[z_n]^T] [\sum_{n=1}^NE[z_nz_n^T]]^{-1}$$
$$\sigma_{new}^2 =  diag ( S -  W_{new} \frac 1 N \sum_{n=1}^N E[z_n](x_n-\hat x) ^ T )$$

## 12.3 核PCA
(略)
## 12.4 非线性隐变量模型
本节讨论非高斯或者非线性模型。

### 12.4.1 独立成分分析
模型的可观测量和隐变量线性相关，但是隐变量为非高斯分布。此时的模型叫**独立成分分析(independent component analysis)**。常见的应用场景：将混合在一起的两个声音分开。

### 12.4.2 自动关联神经网络
这种神经网络可以用来无监督学习降维。方法是将输入层神经元数量等于输出层。误差为输入和输出的差。中间隐藏层拥有比输入/输出更少的神经元。

### 12.4.3 非线性流形建模

