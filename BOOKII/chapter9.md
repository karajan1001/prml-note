# 9.混合模型和EM
对于可观测量和隐变量，可观测量的分布可以通过计算边缘概率积分掉隐变量获得。通过隐变量可以从简单模型构成复杂模型，混合分布比如高斯混合可以被分解为分立的隐变量。不光是提供构成复杂模型的方法，混合模型还可以用来聚类。寻找隐变量模型最大似然估计的通用方法叫EM（最大期望算法）。

## 9.1 k-means 聚类
聚类的目标方程为：

$$J = \sum_{n=1}^N \sum_{k=1}^K r_{nk}||x_n - \mu_k||^2$$
其中$r_{nk}$为 1-of-K 编码。$\mu_k$ 为中心点。k-means 分两步，第一步更新$r_{nk}$ ，第二步更新$\mu_k$。这两步分别对应EM 算法中的 **E(Execptation)** 和 **M(Maximization)**。
K-means算法还可以进行在线更新。其中的欧几里得距离限制了其性能，导致其在处理类目数据和一些其他工作时表现不好，所以可以改进为 **k-medoid**算法。k-means算法的E步骤的复杂度为O(KN)，M步骤为O($N_K^2$)。E步骤每个数据都强制属于一个分类，这导致一些边缘数据比较麻烦。

### 9.1.1 图像分割和压缩
图像分割：将图像点分成K类，然后分别用K类中心替代。
图像压缩：方法和分割类似。有另一个名字（向量量化vector-quantization），而$\mu_k$叫做**code-book vectors**


## 9.2 高斯混合
高斯混合模型可以写成

$$p(x) = \sum_z p(z) p(x|z)= \sum_{k=1}^K\pi_k N(x|\mu_k, \Sigma_k)$$  其中$z_k$ 为 1-of-K表示{0, 0, 0, 0, ... , 1 , 0, 0, 0}$p(z_k = 1 ) = \pi_k $
### 9.2.1 最大似然
对于iid数据

$$ln(p(X| \pi, \mu, \Sigma) = \sum_{n=1} ^N ln\big\{\sum_{k=1}^K\pi_kN(x_N| \mu_k, \Sigma_k )\big\} （9.14）$$
混合高斯分布在方差无限小时，分布概率会无限大。在单高斯分布时不会有这个问题，因为其他点的概率会趋近于0，导致最后的联合概率并不高。这也是一种过拟合，使用贝叶斯理论后会消失。还有另一个问题是：寻找每个类目的标签可以互换，K个类目有K!种等价表示方式。

### 9.2.2 高斯混合EM
EM是一种解高斯混合最大期望的有效方法。

**E** 步骤则是基于上式中的参数估计每个点属于每个k的概率。

$$\sigma(z_{nk}) = \frac{\pi_k N(x_n|\mu_k, \Sigma_k)}{\sum_{j=1}^N \pi_j  N(x_n|\mu_j, \Sigma_j)} (9.23)$$
对 **(9.14)** 求$\mu_k, \Sigma_k, \pi_k$偏导 可以得到 

$$ \mu_k = \frac 1 N_k \sum_{n=1}^N \gamma(z_{nk})x_n (9.24)
 \Sigma_k = \frac 1 N \sum_{n=1} ^ N \gamma(z_{nk}) (x_n- \mu_k)(x_n-\mu_k)^T (9.25)\\\pi_k = \frac {N_k} N (9.26)\\ N_k = \sum_{n=1}^N \gamma(z_{nk}) (9.27)$$
这是 **M** 步骤，最大化最大似然。


EM算法求解高斯混合时要注意：
1. EM算法没有K-means快。
2. 需要注意奇点问题。
3. EM算法不保证收敛到全局最大值。


完整的EM算法步骤：
> 1. 初始化参数。
2. 根据初始参数进行**E**估计。
3. 根据**E**估计最大化参数**M**。
4. 计算最大似然。

`重复如上过程直到最大似然收敛`

## 9.3 关于EM的另一种观点
EM算法的目标是计算包含隐变量的最大似然.。对于一个可观测量为**X**,隐变量为**Z**的模型。其最大似然的log方程为：

$$\ln p(X| \theta) = \ln\big\{ \sum_z p(X, Z|\theta) \big\} (9.29)$$

这个方程的求和在**ln**里面，这导致了直接求导求解的困难。我们将**{X,Z}**对看成完整数据，**X**为不完整数据。求解完整数据的最大似然可以直接得到，而有了后验概率概率$p(Z| X, \theta)$ Z的估计值也能得到。
对于参数$\theta$：有

$$\epsilon(\theta, \theta^{old}) = \sum_zp(Z|X, \theta^{old}) \ln p(X, Z| \theta) (9.30)\\
\theta^{new} = argmax_{\theta} \epsilon(\theta, \theta^{old})(9.31)$$ 

ln直接作用到联合概率分布上。求解器最大值变得更容易。

通用的EM算法步骤为：
> 1. 初始化$\theta^{old}$
> 2. E步骤计算$p(Z|X, \theta^{old})$
> 3. M步骤估计$\theta^{new}$，基于$\theta^{new} = argmax_{\theta} \epsilon(\theta, \theta^{old})$
> 4. 查看是否收敛，如果没有收敛则$\theta^{old} = \theta^{new}$并回到步骤2

EM算法也可以在有先验概率$p(\theta)$的情况下使用，或者用于在X数据存在缺失值的情况下· 。

### 9.3.1 重审高斯混合模型

EM算法将 9. 14式难以对付的  $\ln \sum$ 变成了 $\sum \ln$

### 9.3.2 和 K-means的关系
EM和K-means很相似，不过k-means 在E步骤时使用了硬方法，将每个x指定到一个z上。高斯混合模型如果将方差$\Sigma$设为 0。则会变成了k-means一样的情况。

### 9.3.3 混合 Bernoulli 分布
这个模型又叫隐变量分类分析。这个分析也是隐马尔科夫离散模型的基础。

