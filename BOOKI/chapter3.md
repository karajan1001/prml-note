# 第三章 线性模型回归

## 3.1 线性基函数模型
线性回归方程

$$y(x,w) = w_0 + \sum_{j=1}^{M-1}w_j\phi_j(x) = \vec{w}^T\vec{\phi}(x)$$

其中$\phi_0(x) = 1$ 可以用**基函数**(basis function) 突破线性束缚，拟合非线性结果。
有很多种基函数选择，多项式，高斯，傅里叶，等等。 

### 3.1.1 最大似然与最小平方差
目标变量由$t = y(x,w)$给出，目标被加上一个均值为0，精度为$\beta$的随机变量。则有：

$$p(t|x,w,\beta) = N(t|y(x,w), \beta^{-1})$$

其期望满足$E(t|x) = y(x,w)$。将一组数据集概率联乘，求w的最大期望等价于求最小二乘

$$E_D(w) = \frac 1 2 \sum_{n=1}^N \{t_n - w^T\phi(x_n)\}^2$$
其解为：

$$W_{ML} = (\Phi^t\Phi)^{-1} \Phi^Tt$$

其中$\phi$为一个N*M的矩阵，每行同一个样本，每列同一个坐标基，又被叫做**设计矩阵(design matrix)**

$$\phi =\begin{pmatrix}
 \phi_0(x_0)&  \phi_1(x_0)& \phi_2(x_0)  & ... & \phi_M(x_0)\\ 
 \phi_0(x_1)&  \phi_1(x_1)& \phi_2(x_1)  & ... & \phi_M(x_1)\\ 
 \phi_0(x_2)&  \phi_1(x_2)& \phi_2(x_2)  & ... & \phi_M(x_2)\\ 
 ... & ... & ... & ... & ...\\ 
 \phi_0(x_N)&  \phi_1(x_N)& \phi_2(x_N)  & ... & \phi_M(x_N)
\end{pmatrix} $$

而$(\phi^T\phi)^{-1}\phi^T$被称为$\phi$的伪逆矩阵可以看成逆矩阵在非方阵的推广。 而精度$\beta_{ML}$可以由函数
$$\frac 1 {\beta_{ML}} = \frac 1 N \sum_{n=1}^N \{t_n - w_{ML}^T \phi(x_n)\}^2$$
给出。

### 3.1.2 最小平方的几何表示

可以看成N维空间中，有一个点$t=(t_1,t_2,...t_N)^T$，而每个在数据点$x_n$处的基函数也可以转化为一个点，$\phi_i = (\phi_i(x_1),\phi_i(x_2)...\phi_i(x_N))^T$。M个基函数形成一个子空间y，最小平方的解就是子空间y与t的最短欧式距离。

### 3.1.3 顺序学习

当点数量很大时计算会很困难，所以一般采用*时序/在线*算法比如**stochastic gradient descent**
$$w^{\tau+1} = w^\tau - \eta\triangledown E_n$$

### 3.1.4 正则化最小平方
之前提到可以为误差函数添加正则化控制拟合。此时的误差函数为
$$E_D(w) +\lambda E_W(w)$$
如果添加二次正则化而解出的

$$w = (\lambda I + \phi^T\phi)^{-1}\phi^Tt$$

更一般的情况下添加正则化为$\frac \lambda 2\sum_{j=1}^M|M_j|^q$。当q=1时为**lasso**此时会得到一个稀疏矩阵。q=2更为常用。

### 3.1.5 多个输出
如果用同一组基函数预测多个不同输出。可以得到这个问题可以 被分解为K个独立的回归问题，所以之后本节只会讨论单一目标t的形式。

## 3.2 偏差-方差分解
偏差-方差模型是频率学家的分解方法。在平方差损失函数下，最佳预测为条件期望$h(x)$
$$h(x) = E(t|x) = \int t p(t|x) dx$$
区分决策论中产生的误差，和模型估计中的误差。
$$E[L] = \int \{y(x) - h(x)\}^2p(x) dx + \int \int \{h(x) - t\}^2 dx dt $$
其中第二项与$y(x)$无关，为数据本身噪音引起。因为我们数据量有限只有数据集$D$，无法获得$h(x)$的精确估计，用数据集$D$以及通过参数估计计算得到的$y(x)$为$y(x;D)$。对其求期望$E_D[y(x;D)]$，可以得到最后分解。

$$expected\_loss = (bias)^2 + variance + noise$$
其中：

$$(bias)^2 = \int \{E_D[y(x;D)] - h(x)\}^2p(x) dx$$ 

$$variance= \int E_D[\{y(x;D) - E_D[y(x;D)]\}^2]p(x)dx$$ 

$$noise=\int\{h(x) - t\}^2 p(x,t)dxdt$$

其中噪音为采样带来的差异。偏差为模型和真实分布的差异，方差为选择不同数据集时得到模型不同带来的差异。灵活的模型会带来更小的偏差，但是随着数据集不同模型会有较大的方差。而不那么复杂的模型会带来更小的方差，不过更大的偏差。

> 对于一个复杂模型，其$E_D[y(x;D)]$有着良好的效果，不过需要对多个数据集进行平均，而现实中往往没有那么多数据集。贝叶斯方法的核心正是求平均，不过是对参数的后验分布。

## 3.3 贝叶斯线性回归
贝叶斯方法天生就能避免过拟合，而且可以只依靠训练数据就决定模型复杂性。
### 3.3.1 参数分布
如果参数满足先验概率：

$$p(w) = N(w|m_0, S_0)$$
则加上数据后后验概率为：

$$p(w|t) = N (w|m_N, S_N)$$
如果N=0则退化到先验概率，如果$S_0 = -\infty$也就是先验概率分布在无穷广的空间，则后验概率变为$w_{ML}$。
由第二章以及证明
$$m_N = S_N(S^{-1}_0m_0 + \beta\Phi^Tt)$$
$$S^{-1}_N = S_0^{-1} + \beta \Phi^T\Phi$$
为了简化可以将$m_0$设置为0，可以推出二阶正则化。

### 3.3.2 预测分布
实际应用中，给出w的预测只是获得t预测的一个手段。
$$p(t|\vec{t},\alpha,\beta) = \int p(t|\vec{w},\beta) p(\vec{w}|\vec{t},\alpha,\beta) d\vec{w}$$ 
其中$\vec{t}$为训练数据集。可以得到

$$p(t|\vec{x},\vec{t}, \alpha, \beta ) = N(t|m_N^T\phi(x), \sigma_N^2(x))$$
其中预测的方差
$$\sigma_N^2(x) = \frac 1 \beta + \phi(x)^TS_N\phi(x)$$

第一项为数据集方差，第二项为与$w$关联的不确定性。

### 等效核
对于线性回归，将3.3.1 中的估计的$w$带入得到

$$y(x, m_N) = m^T_N* \phi(x) = \beta \phi(x)^T S_N\phi^T t = \sum_{n=1}^N \beta \phi(x)^T S_N \phi (x_n) t_n$$
所以最后结果为训练集目标变量的线性组合。

$$y(x, m_N) = \sum_nk(x,x_n)t_n$$
满足
$$\sum_{n=1}^N k(x, x_n) = 1$$
而其中的$k(x,x_n)$叫做平滑矩阵或者等价核

> 核函数表示线性回归给出了解决回归问题的另一种方法。我们不引入一组基函数(它隐式 地定义了一个等价的核)，而是直接定义一个局部的核函数，然后在给定观测数据集的条件 下，使用这个核函数对新的输入变量x做预测。这就引出了用于回归问题(以及分类问题)的 一个很实用的框架，被称为高斯过程(Gaussian process)

等价核满足一般的核函数共有的一个重要性质，即 它可以表示为非线性函数的向量ψ(x)的内积的形式，即 $\phi(x) = \beta^{1/2} S_N^{1/2} \phi(x)$

## 3.4 贝叶斯模型选择
贝叶斯估计通过对模型参数进行求和积分处理，而不是点估计。这样可以不用验证集，所有数据都被用于训练，避免了交叉验证中重复多次计算问题。比如支持向量机这就是一个贝叶斯模型。在贝叶斯理论中对于数据集$D$从模型${M_i}$中某个模型生成，$D$包含(x,t)对。则其概率为

$$p(M_i|D) \sim p(M_i)p(D|M_i)$$

其中$p(D|M_i)$模型证据(**model evidence**)又被称为边缘似然(**marginal likeli**)。两个模型的边缘似然相除$p(D|M_i)/p(D|M_j)$又叫贝叶斯系数(**bayes factor**)。一旦给定了后验概率就能知道在给定数据集，给定x的条件下t的概率

$$p(t|x,D) = \sum_{i=1}^Lp(t|x,M_i,D)p(M_i|D)$$

这是一个混合分布，整体的预测为对其中每一项的加权平均。权重就是它们的后验概率。

> 模型空间千千万万，一个简单近似为，选取一个可能性最高的模型自己进行预测，这个做法又叫做模型选择。（这个模型指的是model,不是参数parameter)

此时有，该模型的证据等于该模型下所有参数积分

$$p(D|M_i)=\int p(D|w, M_i)p(w, D) dw $$

假设这个模型的先验概率为$\Delta w_{prior}$宽度的一个尖峰$p(w)\sim \frac 1 {\Delta}$，后验概率为$\Delta w_{posterior}$的一个尖峰，对$w_{MAP}$求积分可即使你看成宽度乘以这个值。

$$p(D) = \int p(D|w)p(w)dw \simeq p(D|w_{MAP})\frac{\Delta w_{posterior}}{\Delta w_{prior}}$$
又因为先验概率分布总是大于后验概率分布，所以$\frac{\Delta w_{posterior}}{\Delta w_{prior}}$总是小于0。取对数对于M个参数

$$p(D) \simeq lnp(D|w_{MAP}) + M ln(\frac{\Delta w_{posterior}}{\Delta w_{prior}})$$
可以M较少，第一项比较小，看到随着参数增加第二项减少。所以贝叶斯模型选择先天性惩罚那些复杂的模型。

更直观的说简单模型生成的数据集集中在少数范围，复杂模型生成的数据集则分布在一个很广的范围。对于中等复杂度的数据集，简单模型无法生成这个数据集这叫欠拟合，而复杂模型相反，可能的数据集过于广泛，所以缺少足够证据证明现有数据确实是来自这个模型。

## 3.5 证据近似
在纯粹的贝叶斯方法中，假设先验概率由超级参数$\alpha, \beta$确定。我们可以通过对超参和参数w积分作出预测，但是通常这种方式没有解析解。

$$p(t|\hat t) = \int \int \int p(t | w, \beta) p(w | \hat t, \alpha , \beta ) p(\alpha, \beta | \hat t) dw d\alpha d\beta$$

证据近似(**evidence approximation**): 一种近似，对w进行积分，通过最大化边缘概率确定$\alpha, \beta$的值。这个方法叫经验贝叶斯，或者叫证据近似。

$$p(t|\hat t) \sim p(t| \hat t, \hat \alpha, \hat \beta)= \int p(t | w, \hat \beta) p(w | \hat t, \hat \alpha , \hat \beta )  dw $$

其中，$\hat \alpha, \hat \beta$ 由最大化概率:

$$p(\alpha, \beta| \hat t) \propto p(\hat t|\alpha,\beta)p(\alpha,\beta)$$  
确定， 如果$\alpha,\beta$的先验概率是均匀的，则等效为最大化$p(\hat t|\alpha,\beta)$
### 3.5.1 证据方程的演化

$$p( \hat t |\alpha, \beta) = \int p(\hat t|w,\beta)p(w|\alpha)dw  $$
$$ p(\hat t | \alpha, \beta) = (\frac{\beta}{2\pi})^{N/2}(\frac{\alpha}{2\pi})^{M/2} \int exp\big\{-E(w)\big\} dw $$
$$ ln(p(t|\alpha,\beta)) = \frac M2 ln\alpha + \frac N2 ln\beta - E(m_N) - \frac12ln|A| - \frac N2 ln(2\pi)$$

### 3.5.2 最大化证据方程
最大化$p(t|\alpha,\beta)$可以解出$\alpha和\beta$，和最大似然方程需要验证集不同$\alpha和\beta$都可以直接从数据集中解出。 `未看明白`
### 3.5.3 参数的有效个数
`未看明白`

## 3.6 固定基的局限性
固定基在数据测量之前就确定，而且会带来维度灾难。不过这些生成的线性基大多具有一定联系，所以实际空间比看起来要小。而且我们可以只使用本地坐标基，这项技术在神经网络和SVM中运用很广。

