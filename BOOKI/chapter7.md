# 第七章 稀疏核机
核模型的一个缺点是，需要对所有数据点进行评估，对训练和预测计算量都有很大的要求。本章介绍的方法，只需要少数几个点。SVM是一个决策机器，不提供后验概率。RVM则是基于贝叶斯方程的，提供后验概率。

## 7.1 最大边缘分类器
感知器算法能够保证在有限步骤之内找到一个解。然而，它找到的这个解依赖于w和b的(任意的)初始值选择，还依赖于数据点出现的顺序。如果有多个能够精确分类训练数据点的解，那么应该尝试寻找泛化错误最小的那个解。支持向量机解决这个问题的方法是:引入边缘(margin)的概念，这个概念被定义为决策边界与任意样本之间的最小距离。

在SVM中，决策边界被选择为使得边缘最大的那个决策边界。假如使用高斯核可以证明，随着$\sigma^2$的减小，最优超平面是有最大边缘的超平面，极限情况下，超平面与非支持向量无关。 

每个数据点$x_n$与超平面$y(x) = 0$之间的距离为
$$\frac {t_n(w^T\phi(x)+ b)}{||w||}$$
边缘由距离超平面最近的点给出
$$arg max_{w,b} \big\{\frac 1 {||w||} min_n[t_n(w^T\phi(x_n) + b )]\big\}$$
直接求解较为复杂，所以将其改写为满足约束
$$t_n(w^T\phi(x_n) + b ) \geq 1$$ 
满足等式的点叫做激活点，其他点为未激活点，此时可以将原方程改写为求解
$$arg max_{w,b} \big\{ \frac 1 2 {||w||^2} \big\}$$ 
在约束下的解。求解过程使用拉格朗日方程可以得在约束$a_n \geq 0$下有
$$L(w,b,a) = \frac 1 2 ||w||^2 - \sum_{n=1}^Na_n\{t_n(w^T\phi(x_n)+b)-1\}$$
每个数据点有一个拉格朗日约束，对$w$和$b$求倒数使其等于0，可以得到
$$w = \sum_{n=1}^N a_n t_n\phi(x_n)$$
$$0 = \sum_{n=1}^N a_n t_n$$
反带入可以推得拉格朗日方程的对偶表示
$$L(a) = \sum_{n=1}^Na_n - \frac 1 2  \sum_{n=1}^N \sum_{m=1}^Na_na_mt_nt_mk(x_n, x_m) $$
限制条件为
$$ a_n \geq 0 $$
$$  \sum_{n=1}^N a_nt_n = 0$$
其中$k(x_n, x'_m) = \phi(x_n)\phi(x'_m)$使用核方法可以有效将空间扩展，最后求解也可以表示为核函数$y(x)=\sum_{n=1}^Na_nt_nk(x, x_n) + b$

求解这个函数满足KKT条件
$$an \geq 0 $$
$$ t_ny(x_n)-1 \geq 0 $$
$$ a_n{t_ny(x_n) -  1 } = 0$$
对每个数据点要么$a_n = 0$ 为非支持向量。要么$t_ny(x_n) - 1 = 0$ 为支持向量。支持向量为处于超平面上的点，一旦训练完毕，非支持向量就可以被丢弃。解决二次规划，找到所有支持向量后，可以通过公式
$$t_n(\sum_{m\epsilon S}a_mt_mk(x_n,x_m) + b) = 1$$
求出$b$的值，其中$S$是支持向量的集合。
$$b = \frac 1 {N_S} \sum_{n\epsilon S}(t_n - \sum_{m\epsilon S} a_n a_m t_n t_m k(x_n, x_m))$$

### 7.1.1 重叠类分布
对于有数据重叠的情况下，虽然SVM求得的解是非线性的，但是对训练数据的精确划分会导致泛化效果不佳。此时我们需要修改SVM，允许一些值被分错。此时需要引入一个惩罚项，**松弛变量(slack variable)**$\xi_n$，其随着与分类边界距离增大而增大。对于正确分类的点 $\xi_n = 0$，对其他点$\xi_n = |t_n - y(x_n)|$ 的点位于边缘内部，但是在决策边界的正确一侧有$\xi <1$, 分类错误的点有$\xi > 1$。这样限制条件
$$t_n(w^T\phi(x_n) + b) \geq 1 - \xi_n$$
其中$\xi_n \geq 0$最小化目标变成了
$$C\sum_{n=1}^N \xi_n + \frac 1 2 ||w||^2$$
其中C控制了分类错误点的数量。此时的拉格朗日函数为：
$$L(w, b,\xi, a, \mu) = \frac 1 2 ||w||^2 + C \sum_{n=1}^N\xi_n - \sum_{n=1}^N a_n(t_ny(x_n) - 1 + \xi) -\sum_{n=1}^N \mu_n \xi_n$$
其中 $a_n \geq 0$ 和 $\mu_n \geq 0$是拉格朗日乘数。对应的KKT条件为
$$a_n \geq 0$$
$$t_ny(x_n) -1 + \xi_n \geq 0$$
$$a_n(t_ny(x_n) - 1 + \xi_n) = 0$$
$$\mu_n \geq 0$$
$$\xi_n \geq 0$$
$$\mu_n \xi_n \geq 0$$
求偏导数可以得到
$$w = \sum_{n=1}^N a_nt_n\phi(x_n)$$
$$\sum_{n=1}^Na_nt_n = 0$$
$$a_n = C - \mu_n$$
消除$w,b,\xi_n$最后得到
$$L(a) = \sum_{n=1}^N a_n - \frac 1 2 \sum_{n=1}^N\sum_{m=1}^N a_na_mt_nt_mk(x_n,x_m)$$
其中对偶函数满足约束
$$0\leq a_n \leq C$$
$$\sum_{n=1}^N a_n t_n = 0$$
对于新点的预测与之前结果一样。而最终解中$a_n = 0$的点为支持向量，其他点则满足$t_ny_n = 1 -  \xi_n$ 而$a_n = C$的点位于边缘内部，而$a_n < C$的点有$\xi_n > 0$其他位于边缘之上。最后可以求得b有:
$$b = \frac 1 {N_M} \sum_{n\epsilon M}(t_n - \sum_{m\epsilon M}a_m t_m k(x_n, x_m))$$
支持向量机的另一个等价形式被成为v-SVM。虽然输入预测只需要边缘向量就能求出，但是求解整个过程需要计算所有数据点，因为限制条件定义了一个凸区域，任意局部最优解也是全局最优解。一种最流行的训练支持向量机的方法被称为顺序最小化优化(sequential minimal optimization)，或者称为SMO(Platt, 1999)。这种方法考虑了分块方法的极限情况，每次只考虑两个拉格朗日乘数。这种情况下，子问题可以解析地求解，因此避免了数值二次规划。选择每一步骤中需要考虑的拉格朗日乘数对时，使用了启发式的方法。在实际应用中，SMO与训练数据点数量的关系位于线性与二次之间，取决于具体的应用。 SVM无法计算只提供分类面，因为其建模不涉及概率计算，所以将其作为大系统中一部分输出概率的效果会很差。

### 7.1.2 与logistic回归的关系
SVM的误差函数可以写作:
$$\sum_{n=1}^N E_{SV}(y_nt_n) +\lambda ||w||^2$$
其中$E_{SV}(y_nt_n) = [1 - y_n t_n]_+$，其中$[]_+$表示正数部分,而logistic回归则写作
$$\sum_{n=1}^N E_{lr}(y_nt_n) +\lambda ||w||^2$$
其中$E_{LR}(y_nt_n) = ln (1 + exp(-y_nt_n))$与SVM的区别在于SVM的损失函数产生了稀疏性。

### 7.1.3 多类svm
常用的方法为`1对剩余`但是这个方法会产生不相容结果，而$y(x) = max_k y_k(x_n)$的方法无法保证每个分类起的数值之间具有可比较性。另一个方法是训练`1对1`的方法，不过这个方法可能会产生歧义性。总的来说目前`1对剩余`还是SVM中使用最多的多分类方法。

### 7.1.4 回归svm
`略`

### 7.1.5 计算学习理论
历史上，支持向量机大量地使用一个被称为`计算学习理论(computational learning theory)`的理论框架进行分析。这个框架有时候也被称为统计学习理论

## 7.2 相关向量机
支持向量机还是有许多局限性，SVM的输出是一个决策结果而不是后验概率。并且，SVM最开始用于处理二分类问题，因此推广到K > 2类有很多问题。有一个复杂度参数C或者ν(以及回归问题中的参数ε)必须使用诸如交叉验证的方法确定。最后，预测是用核函数的线性组合表示的，核函数以训练数据点为中心，并且必须是正定的。
**相关向量机(relevance vector machine)**或者RVM是一个用于回归问题和分类问题的贝叶斯稀疏核方法，它具有许多SVM的特征，同时避免了SVM的主要的局限性。此外，它通常会产生更加稀疏的模型，从而使得在测试集上的速度更快，同时保留了可比的泛化误差。

### 7.2.1 用于回归的RVM
假设输入向量为x，实值目标变量t的分布为
$$p(t|x,w,\beta) = N(t |y(x) , \beta ^{-1})$$
均值由线性模型
$$y(x) = \sum_{i=1}^Mw_i\phi_i(x)$$
给出。参数$w$的概率由
$$p(w|\alpha) = \sum_{i=1}^N N(w_i|0, \alpha_i^{-1})$$
由第三章可以知道权值的后验概率依然是高斯分布。
$$\alpha_i^{new} = \frac {\gamma_i}{m_i^2}$$
$$(\beta_i^{new})^{-1} = \frac {||t -\Phi m||^2}{ N- \sigma_i \gamma_i}$$
$$\gamma_i = 1 - \alpha_i\Sigma_{ii}$$
可以使用迭代方法求$\alpha$和$\beta$，或者用EM求解。解中有一部分$\alpha_i$会特别大，应该予以去掉，而剩下的就是相关向量。RVM的相关向量比SVM支持向量小一个量级。

### 7.2.2 稀疏性分析
`略`
### 7.2.3 RVM用于分类

`略`








