# 第六章 核方法
之前的方法中，训练过程一般使用训练数据来获得参数，而在预测过程中，抛弃训练数据，而只用参数来进行预测。而在本章中，训练数据(或者一部分)在预测过程中也被使用，比如最近邻方法。这类方法需要一个用来计算样本相似度的**核函数**，通常他们的训练过程非常快，但是预测很耗时。很多线性参数方法都可以被转换成对偶表示，最简单的**核函数**是$k(x, x') = x^Tx'$ 。核方法基本思想为，如果我们有一个算法方程中的x只在标量乘积中出现，则我们可以将其替换为其他核函数。有两种特殊核函数：

1. **静止核（stationary kernels）**：核函数只与相对位置有关$k(x, x') = k(x - x')$
2. **同类核（homogeneous kernels）**：核函数只与相对距离有关$k(x, x') = k(|x - x'|)$

## 6.1 对偶表示
对于一个正则化的平方和误差：
$$J(w) = \frac 1 2 \sum_{n=1}^ N {w^T\phi(x_n) - t_n}^2 + \frac \lambda 2 w^Tw$$
对于$\frac{dJ}{dw} = 0$时有 
$$w = \frac 1 \lambda \sum_{n=1}^N{W^T\phi (x_n)-t_n}\phi (x_n) = \Phi^Ta_n$$
其中有（$\lambda w^Tw$的微分在左边）
$$a_n = \frac1 \lambda {w^T\phi(x_n) - t_n}$$
所以有
$$J(a) = \frac1 2 a^TKKa - a^TKt + \frac1 2 t^Tt + \frac \lambda 2 a^T K a$$
其中
$$K_nm = \phi(x_n)^T\phi(x_m) = k(x_n, x_m)$$
对a求解可以得到
$$a = (K + \lambda I_N)^{-1}t$$
对新的x可以得到
$$y(x) = w^T\phi(x) = a^T\Phi\phi(x) = k(x)^T(K+\lambda I_N)^{-1}t$$
最小二乘方程和其解可以完全由核函数k表示而不需要通过显式求出w。虽然N的维度要比M大的多，但是a可以靠核函数扩展到很高甚至的无限维度。

## 6.2 构建核函数
两种方法一种选择合适的坐标基
$$k(x,x') = \sum \phi (x)^T \phi (x')$$

第二种直接构造核函数，此时有效核函数要求是Gram矩阵，在所有元素选择下都是半正定的。比如高斯核，这种情况下核函数维度是无穷的。
$$k(x,x') = exp(-\frac {||x - x'||^2}{2\sigma^2})$$
核还可以扩展到符号化输入比如：集合符号。

## 6.3 RBF网络
RBF 方程是指的那些，之和欧几里得距离相关的方程。对于基函数是指的：
$$\Phi_j(x) = h(|x - \mu_j|)$$
使用这样核函数的模型可以看做以每个数据点为一个坐标基。向量大小等于$t_n$。因为数据点数量太多，所以实际使用时要选取特殊数据点进行。

### 6.3.1 Naaraya-Watson 模型
使用Parzen密度估计函数有：
$$p(x,t) =  \frac 1 N \sum f(x-x_n,t-t_n)$$
所以有
$$y(x) = E(t|x) = \int t p(t|x) dx = \frac {\sum_n\int tf(x-x_n,t-t_n)}{\sum_m \int f(x-x_m, t-t_m)}$$
假设$g(x)=\int f(x,t)dt$有
$$k(x, x') = \frac {g(x-x_n)}{\sum_m g(x-x_m)}$$
$$y(x) = \sum_n k(x, x') t_n$$

## 6.4 高斯过程
在高斯过程的观点中，我们抛弃参数模型，直接定义函数上的先验概率分布。虽然在函数组成的不可数的无穷空间中对概率分布进行计算似乎很困难。但是，对于一个有限的训练数据集，只需要考虑训练数据集和测试数据集的输入$x_n$处的函数值即可，因此在实际应用中我们可以在有限的空间中进行计算。

### 6.4.1 重新考虑高斯过程
对于一个参数服从高斯先验分布线性模型$y(x) = w^T \phi(x)$有:
$$p(w) = N(w | 0 , \alpha^{-1}I)$$
可以求出
$$E(y) = \phi E[w] = 0$$
$$cov(y) = K$$
**高斯过程**：指的是一个方程$Y(x)$的概率分布，在任意数据集$x_1, x_2 ... x_N$上的联合概率分布为联合高斯分布。高斯过程中的N个值$y_1, y_2 ... y_N$ 由 均值和方程完全决定，多数情况下，均值为0，方差为由核函数确定。不同过程对应不同核函数，高斯核函数为

$$K_{nm} = k(x_n, x_m) = \frac 1 \alpha \phi(x_n)^T \phi(x_m)$$
### 6.4.2 用户描述回归的高斯过程

下面两个式子定义任意核函数的高斯回归方程：
$$m(x_{N+1}) = k^TC_N^{-1}t$$
$$\sigma(x_{N+1}) = c - k^TC_N^{-1}k$$

### 6.4.3 学习超参数

高斯过程模型的预测部分依赖于协方差函数的选择。在实际应用中，我们不固定协方差函数，而是更喜欢使用一组带有参数的函数，然后从数据中推断参数的值。这些参数控制了相关性的长度缩放以及噪声的精度等等，对应于标准参数模型的超参数。

为了对这种问题进行建模，我们可以对高斯过程框架进行推广，引入第二个高斯过程来表示$\bate$对于输入x的依赖性。由于$\beta$是一个方差，因此是非负的，所以我们使用高斯过程来对$\ln\beta(x)$进行建模。

### 6.4.4 自动确定相关性
通过为每个输入变量整合一个单独的参数，这种方法可以很有用地推广正如我们将看到的那样，这样做的结果是，通过最大似然方法进行的参数最优化，能够将不同输入的相对重要性从数据中推断出来。这是高斯过程中的自动相关性确定或者ARD的一个例子。

### 6.4.5 用于确定分类的高斯过程
可以考虑三种不同的获得高斯近似的方法:
1. 基于变分推断
2. 使用期望传播
3. 拉普拉斯近似

### 6.4.6 拉普拉斯近似
略

### 6.4.7 与神经网络的联系

神经网络可以表示的函数的范围由隐含单元的数量M控制，并且对于足够大的M，一个两层神经网络可以以任意精度近似任意给定的函数。在最大似然的框架中，隐含单元的数量需要有一定的限制来避免过拟合现象。然而，从贝叶斯的角度看，根据训练集的规模限制参数的数量几乎毫无意义。
在贝叶斯神经网络中，参数向量$w$上的先验分布以及网络函数$f(x, w)$产生了函数$y(x)$上的先验概率分布，其中y是网络输出向量。在极限$M\sim \inf$的情况下，对于$w$的一大类先验分布，神经网络产生的函数的分布将会趋于高斯过程。然而在这种极限情况下，神经网络的输出变量会变为相互独立。神经网络的优势之一是输出之间共享隐含单元，因此它们可以互相“借统计优势”，即与每个隐含结点关联的权值被所有的输出变量影响，而不是只被它们中的某一个影响。这个性质在极限状态下的高斯过程中丢失了。