

## 核方法
之前的方法中，训练过程一般使用训练数据来获得参数，而在预测过程中，只是用参数获得预测值。而在另一类方法中，训练数据（*或者一部分*）在预测过程中也被使用。比如最近邻方法，这类方法需要一个用来计算样本相似度的**核函数**，通常他们的训练过程非常快，但是预测缺很耗时。很多参数方法都可以被转换成对偶表示，最简单的**核函数**是$k(x, x') = X^Tx'$ 。核方法基本思想为，如果我们有一个算法方程中的x只在标量乘积中出现，则我们可以将其替换为其他核函数。有两种特殊核函数：
1. **静止核（stationary kernels）**：核函数只与相对位置有关$k(x, x') = k(x - x')$
2. **同类核（homogeneous kernels）**：核函数只与相对距离有关$k(x, x') = k(|x - x'|)$

### 对偶表示
对于一个最小二乘方程$$J(w) = \frac 1 2 \sum_{n=1}^ N {w^T\phi(x_n) - t_n}^2 + \frac \lambda 2 w^Tw$$
对于$\frac{dJ}{dw} = 0$时有 $$w = \frac 1 \lambda \sum_{n=1}^N{W^T\phi (x_n)-t_n}\phi (x_n) = \Phi^Ta$$
则有$$a_n = \frac1 \lambda {w^T\phi(x_n) - t_n}$$
所以有$$J(a) = \frac1 2 a^TKKa - a^TKt + \frac1 2 t^Tt + \frac \lambda 2 a^T K a$$其中$$K_nm = \phi(x_n)^T\phi(x_m) = k(x_n, x_m)$$
对a求导数可以得到$$a = (K + \lambda I_N)^{-1}t$$
对新的x可以得到$$y(x) = w^T\phi(x) = a^T\Phi\phi(x) = k(x)^T(K+\lambda I_N)^{-1}t$$
最小二乘方程和其解可以完全由核函数k表示而不需要通过w。虽然a的维度要比m大的多，a可以靠核函数扩展到很高甚至的无限维度。

### 构建核函数

### RBF网络
RBF 方程是指的那些，之和欧几里得距离相关的方程。对于基函数是指的：
$$\Phi_j(x) = h(|x - \mu_j|)$$
使用这样核函数的模型可以看做以每个数据点为一个坐标基。向量大小等于$t_n$。因为数据点数量太多，所以实际使用时要选取特殊数据点进行。

#### Naaraya-Watson 模型

`参见3.3 未看懂`。

### 6.4 高斯过程
**高斯过程**：指的是一个方程$Y(x)$的概率分布，在任意数据集$x_1, x_2 ... x_N$上的联合概率分布为联合高斯分布。高斯过程中的N个值$y_1, y_2 ... y_N$ 由 均值和方程完全决定，多数情况下，均值为0，方差为由核函数确定。不同过程对应不同核函数，高斯核函数为$$K_{nm} = k(x_n, x_m) = \frac 1 \alpha \phi(x_n)^T \phi(x_m)$$
未看懂
