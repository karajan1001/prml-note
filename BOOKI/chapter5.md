

## 神经网络（参考深度学习）
固定基的线性回归和线性分类需要手动指定坐标基，SVM首先定义从每个训练数据为中心的基，然后从中选择其中的一个子集，神经网络则是在先设定一组坐标基，然后再训练过程中自适应。神经网络模型比SVM更为紧凑，付出的代价是，神经网络是非凸优化
### 前向传播方程
前向传播方程为：
$$a_j = \sum_{i=1}^Dw_{ji}^{(1)} + w_{j0}^(1) \\ z_j = h(a_j) \\ a_k = \sum_{j=1}^Dw_{jk}^{(2)} + w_{k0}^(2)
\\ ... $$ 
其中$x_1 ,...x_D$ 是D 个输入，$a_1,...,a_M$ 是M 个激活，$w_ji$ 是需要训练的权重$(1), (2)$ 代表第一层或者第
二层。

虽然叫多层感知器，但是和感知器不同，神经网络是连续函数，所以可以微分。如果中间元的数量小于输入或者输出单元，则产生的神经网络不是通用性的，因为信息在传递过程中损失掉了。

神经网络可以模拟任意函数，而且和贝叶斯模型不同，对同样输入输出对，会有多种不同的参数选择可能。比如调换一个隐藏单元的所有输入和输出权重，模型输出不变，M个隐藏单元有$2^M$个等效解。兑换同层两个隐藏单元，模型输出不变，M个隐藏单元有$M!$个等效解

### 网络训练
对于回归可以使用最小二乘作为误差方程。而对于分类问题，二分类可以使用logistic sigmoid方程作为输出，多分类使用softmax 输出，然后选择交叉熵作为误差方程。
#### 参数优化
神经网络每个最小值都有多个等价最小值，还有很多不等价局部极小值。全局最小值一般无法找到，所以优化方法为迭代优化寻找局域极小值。

#### 局域二次近似
对误差方程进行泰勒展开：$$E(W) \simeq E(w_0) + (w -w_0)^Tb + \frac 1 2 (w - w_0)H(w-w_0)$$
在局部极小值有 $$E(w) = E(w^*) + \frac 1 2 (w - w^*) H (w - w^*)$$
本征值分解 $$Hu_i = \lambda_i u_i \\ E(w) = E(w^*)+\frac1 2 \sum_i \lambda_i \alpha_i^2$$
因为是极小值所以每个$\lambda$都为正。H为**正定positive definite**矩阵。

#### 梯度下降优化
相比使用batch梯度下降，随机梯度下降更快，而且不容易掉到局部最小值中

### 反向传播
计算使用反向传播比较快，但是验证的时候使用数值微分，这样准确率高。

### Hessian 矩阵
Hessian矩阵二阶导数的作用：
1. 辅助一些优化算法
2. 训练集一些小变化时的重训练算法理论基础。
3. 用来发现不那么重要的参数，进行网络纯化。
4. 贝叶斯网络拉普拉斯近似的

计算方法和反向传播算法类似。

#### 对角近似
计算方便可以把复杂度从$O(W^2)$降低到$O(W)$。

#### 外积近似
$$H = \nabla \nabla E  = \sum_{n=1}^N \nabla y_n \nabla y_n + \sum_{n=1}^N (y_n - t_n) \nabla \nabla y_n$$
第二项在$y_n - t_n \simeq 0$ 时还有误差随机互相抵消时可以近似为0。这样近似后$$H  \simeq \sum_{n=1}^N b_n b_n^T$$ 其中$b_n = \nabla y_n$这种近似只有在充分训练后的网络才有效。

#### Hessian 逆矩阵
计算反向传播的时候可以顺便迭代更新计算Hessian矩阵。
$$H^{-1}_{L+1} = H^{-1}_{L} - \frac{H_L^{-1}b_{L+1}b^T_{L+1}H_{L}^{-1}}{I + b_{L+1}^TH_L^{-1}b_{L+1}}$$

#### 计算
`暂时没看`

### 正则化
减少模型复杂度的方法，最简单的方法是二次正则，$E(W) = E(W) + \frac \lambda 2 W^T W$

#### 高斯先验一致性
如果使用简单的二次正则，则对某一层做线性变换则，会导致训练出不同结果。如果要求线性一致性，则必须每一层有单独的正则化常数。

#### Early Stopping
训练时，训练集的训练误差一致下降，而测试集的误差先是下降，后来因为过拟合而上升。Early Stopping得道的效果和参数衰减类似

#### 不变性
有时候，分类问题有很多不变性，比如图像识别中图像的拉伸，旋转，平移等。
解决不变性的方法有四种：
1. 制造一些变换后的样本。
2. 增加专门的正则化，对不满足不变性的特征进行惩罚性衰减。
3. 手动选择满足不变性的特征。
4. 建立满足不变形的模型。

#### 切线方法

某个变换下，x变换的切线方向为$$\tau_n = \frac{ds(x_n, \epsilon)}{d\epsilon}|_{\epsilon = 0}$$最终结果在切线方向的倒数为$$ \frac{dy_k}{d\epsilon} |_{\epsilon=0} = \sum_{i=1}^D J_{ki}\tau_i$$增加正则项$\hat{E} = E  +\lambda \Omega$其中$$\Omega = \frac 1 2 \sum_n \sum_k (\sum_{i=1}^D J_{nki}\tau_{ni})^2$$

可以从数学上证明这个方法和制造变换后样本等价。（证明方法，变换后的数据泰勒展开后等于增加了正则项）

#### 卷积神经网络满足一些不变性

#### 混合密度网络

有时候，数据有多模式，此时用简单回归方法用均值去预测值就会出现较大误差。 此时可以训练混合密度网络，对每种模式求均值作为估计。

#### 贝叶斯神经网络

`未看明白`
