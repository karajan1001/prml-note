
## 线性模型分类
分类问题是把输入映射到离散的类别中。分类依赖决策区域或者又叫**决策边界（de cision boundaries）**。而线性分类模型是指的，分类边界是输入的线性函数，也就是说是D维输入空间中由D-1个变量决定的一个超平面。在二分类问题中一般使用0，1来表示分类。而多分类问题中，更多的是使用1-of-K coding表示。对于线性回归问题。有$$y(x) = w^Tx + w_0$$，对于线性回归问题，我们可以使用
$$y(x) = f(w^Tx + w_0)$$这里的$f(x)$被称为**激活函数(activation function)**
### 判别式方程
线性判别式方程是空间中一个超平面，平面方程大于0和小于0对应不同的分类。对多分类问题如果从二分类扩展，一个可行的方法是N个方程$$y_k(x)=w_k^Tx+w_{k0}$$对任意一个点如果第K个方程值最大，则它属于分类K。这个方程组形成的分类区域是联通而且凸的。
#### Fisher的线性判别式
将数据投影到一维平面，fisher的判别式方程寻找一个投影方向，最大化类平均值的间距的同时，最小化类方差。具体方程为$$J(w) = \frac{(m_2-m_1)^2}{s_1^2+s_2^2}$$
其解$w \propto S_W^{-1}(m_2-m_1)$

#### Fisher是最小二乘的特例
如果设定$$t_{c_1} = \frac N{N_1} \\ t_{c_2} = \frac N{N_2} $$ 最小二乘可以推导出Fisher判别式。其中N是总样本，$N_1和N_2$分别为1，2类别的样本量

#### 感知器算法
先将输入x转化为一组坐标基$\phi(x)$ 非线性方程组然后$$y(x) = f(w^T \phi(x))$$，其中f是非线性激活函数。读线性可分数据，感知器有限步必然会收敛，但是如果数据不可分，则不会收敛。而且感知器对$K>2$的情况没法使用

### 概率生成模型
**logistic sigmoid** 方程的由来
$$\sigma(a)  = \frac1{1+exp(-a)} $$ 假如 $$ a = ln\frac{p(x|c_1)p(c_1)}{p(x|c_2)p(c_2)}$$
则可以得到 $$p(c_1|x) = \sigma(a)$$

#### 连续输入

如果输入满足高斯分布$$p(x|C_k) = \frac{1}{(2\pi)^{D/2}}\frac{1}{(\Sigma)^{1/2}}exp\big \{ -\frac1 2(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \big\}$$ `这里假定了每一个Class的方差矩阵$\Sigma都相同，不然会是一个二次方程而不是线性方程$`
可以得到K类的线性方程：
$$a_k(x) = w_k^Tx + w_{k0} \\ w_k = \Sigma^{-1}\mu_k \\ w_{k0} = -\frac12\mu_k^T\Sigma^{-1}\mu_k + ln p(C_k)$$

#### 最大似然解
可以通过最大似然估计，求出上述连续分布下在固定模型时最大似然情况下模型参数的值。

### 概率判别模型
概率判别模型比概率生成模型简单，分类时准确率更高。

#### 固定基方程
非线性固定基可以使得某些基下不可分的方程线性可分。但是某些情况时不同类型的点混在一起，就算是非线性变换也不能把它们分开。

#### 逻辑回归
对高斯生成模型，$M$个参数需要$M$个均值和$M^2$个协方差。而逻辑回归只要M个参数。对高斯生成模型，迭代一次就能到最小值，而对于逻辑回归每次迭代迭代量都会改变，只能逼近最小值。

两分类概率方程为 ：$$p(t|w) = \prod_{n=1}^N y_n^{t_n}{1-y_n}^{1-t_n}$$
误差方程为：$$E(w) = -lnp(t|w) = \sum_{n=1}^N \big\{t_n*y_n + (1-t_n)*(1-y_n)\big\}$$
多分类概率方程为：
多分类误差方程为**交叉熵方程(cross entropy)**：$$E(w_1,w_2...w_k) = -lnp(T|w_1,w_2...w_k) =  \sum_{k=1}^K  \sum_{n=1}^N y_{nk}^{t_{nk}}$$

**probit regression**：很像逻辑回归，由一个均值为0，方差为1的高斯分布积分得来。


#### 正则连接方程(canonical link functions)
高斯分布的log似然方程对第n个点的导数同样可以从逻辑回归 交叉熵方程对第n个点求导得出，更一般只要是指数家族都可以

### 拉普拉斯近似
未知分布，用高斯分布去近似，均值为最大值位置，精度为最大值位置的二阶导数。

#### 模型比较和BIC
 *Bayesian information criterion (BIC)*：将拉普拉斯近似求最大概率的导数，可以得到$$lnp(D) \simeq lnp(D|\theta_{MAP}) + lnp(D|\theta_{MAP}) + \frac M 2ln(2\pi) - \frac 1 2 ln |A| $$

### 贝叶斯逻辑回归

使用拉普拉斯近似可以得到：后验概率为$$q(w) = N(w|w_{MAP}, S_N)$$

给定t预测$C_1$的概率分布：得到$$p(c_1|t) = \int \sigma(a)p(a)da = \int\sigma(a)N(a|\mu_a,\sigma_a^2)da$$
使用上文提到的**probit regression**可以得到

$$\int\sigma(a)N(a|\mu,\sigma^2)da \simeq \sigma(k(\sigma^2)\mu) \\
k(\sigma^2) = (1+\frac{\pi\sigma^2}8)^\frac 1 2$$
