# 第四章 线性模型分类
分类问题是把输入映射到离散的类别中。每个输入被分到唯一的一个类别中，输入空间按照所分类别被划分为不同的**决策区域(decision region)**,区域之间的边界叫**决策边界（decision boundaries）**。而线性分类模型是指的，分类边界是输入的线性函数，也就是说是D维输入空间中由D-1个变量决定的一个超平面。在二分类问题中一般使用0，1来表示分类。而多分类问题中，更多的是使用1-of-K coding表示。对于线性回归问题。有
$$y(x) = w^Tx + w_0$$
预测的结果为一个实数。对于线性回归问题，我们希望预测的结果是一个离散标签，或者更一般的是（0，1）之间的一个概率值，此时我们可以使用推广
$$y(x) = f(w^Tx + w_0)$$
这里的$f(x)$被称为**激活函数(activation function)**它的反函数在统计文献中被成为**链接函数(link function)**,决策面对应$y(x)$等于常数。带有激活函数的模型被成为推广线性模型，但是它们其实是非线性的，因为激活函数往往不是线性的。
## 4.1 判别函数
判别函数是一个函数，给定一个输入x将其分配到K个类目中的一个的函数。本节注意力集中于线性判别函数或者是决策面是超平面的判别函数。

### 4.1.1 二分类
判别函数中最简单的形式是输入向量的线性函数。

$$y(x) = w_T x + w_0$$
其中，$w_T$被称为**权向量(weight vector)**，而$w_0$叫**偏置(bias)**，它的相反数叫做**阈值(threshold)**。对于一个输入当$y(x)>0$它属于$C_1$ 反之属于$C_2$。决策分界面由$y(x)=0$确定。因为对于决策平面上任意点$x_A,x_B$都有$w_T(x_A-x_B)=0$所以对于决策平面上的任意向量都与$w_T$正交。任意一点到超平面的距离为$\frac{y(x)}{||w_T||}$对于原点，到超平面的距离为$\frac{w_0}{||w_T||}$。引入一个额外输入$x_0=1$则公式可以化简为
$$y(x) = \hat w_T \hat x$$

### 4.1.2 多分类

对多分类问题如果从二分类扩展，one-versus-one 或者 one-versus-the-rest 方法都会导致某些区域要么类别无法确定，要么分类冲突，一个可行的方法是N个方程。
$$y_k(x)=w_k^Tx+w_{k0}$$
对任意一个点如果第K个方程值最大，则它属于分类K。
可以证明，这个方程组形成的分类区域是联通而且凸的。

### 4.1.3 最小二乘
如果对分类问题使用类似回归问题的最小二乘法可以得到类似的解。但是这种解法有两个问题：
1. 输出$y_k(x)$不在[0, 1]区间内，也不满足所有类目的概率之和为1。
2. 第二，缺少鲁棒性，对于过于正确的点会引起分界面明显的错误变化。
最小二乘法的问题在于，分类问题预测值为0，1明显不满足高斯分布。

### 4.1.4 Fisher的线性判别式

将数据投影到一维平面，fisher的判别式方程寻找一个投影方向，最大化类平均值的间距的同时，最小化类方差。具体方程为
$$J(w) = \frac{(m_2-m_1)^2}{s_1^2+s_2^2}$$
其中$m_1,m_2$ 为两个类的均值向量
$$m_1= \frac 1 {N_1} \sum_{n \epsilon C_1} x_n;
m_2= \frac 1 {N_2} \sum_{n \epsilon C_2} x_n$$
$$s_1= \sum_{n \epsilon C_1}(x_n - m_1)^2;s_2= \sum_{n \epsilon C_2}(x_n - m_1)^2$$

这个式子可以化为
$$J(w) = \frac {w_TS_Bw} {w_TS_Ww}$$
其中
$$S_W = \sum_{n\epsilon C_1} (x_n-m_1)(x_n - m_1)^T + \sum_{n\epsilon C_2} (x_n-m_2)(x_n - m_2)^T $$
$$S_B = (m_2-m_1)(m_2-m_1)^T$$

其解为$w \propto S_W^{-1}(m_2-m_1)$

### 4.1.5 Fisher是最小二乘的特例
如果设定两个类目的目标值为
$$t_{c_1} = \frac N{N_1} ; t_{c_2} = -\frac N{N_2} $$ 
对这两个目标值使用最小二乘则可以推导出Fisher判别式。其中N是总样本，$N_1$和$N_2$分别为1，2类别的样本量。

### 4.1.6 多分类的Fisher判别函数
将Fisher判别函数推广到N分类问题。可以得到
$$J(w) = Tr\{(W^TS_WW)^{-1}(W^TS_BW)\}$$

### 4.1.7 感知器算法
先将输入x转化为一组坐标基$\phi(x)$ 然后利用激活函数将线性函数$w^T\phi(x)$

$$y(x) = f(w^T \phi(x))$$

其中f是非线性激活函数。满足$f(x) = 1$当$x>0$时$f(x) = -1$当$x<0$时。 但是这个函数不可导，所以要引入额外的误差函数
$$E_p(w) = - \sum_{n\epsilon M}w^T\phi_n t_n$$ 
其中$t$对不同类别分别等于正负1。对误差使用梯度随机下降。
读线性可分数据，感知器有限步必然会收敛，但是如果数据不可分，则不会收敛。而且感知器对$K>2$的情况没法使用

## 4.2 概率生成模型
利用贝叶斯定力有
$$p(C_1|x) = \frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1) + p(x|C_2)p(C_2)}$$
假设
$$ a = ln\frac{p(x|c_1)p(c_1)}{p(x|c_2)p(c_2)}$$
则可以得到**logistic sigmoid** 方程:
$$ p(C_1|x) = \sigma(a)  = \frac1{1+exp(-a)} $$ 
sigmoid函数，意义为s形，或者叫挤压函数。
$$\sigma(-a) = 1 - \sigma(a)$$
而它的反函数
$$a=\ln(\frac{\sigma}{1-\sigma})$$
被称为logit函数表示两个类概率比值的对数。
对于多分类问题有:
$$p(C_k|x) = \frac{p(x|C_k)p(C_k)}{\sum_j^N p(x|C_j)p(C_j)}$$
$$p(C_k|x) = \frac{exp(a_k))}{\sum_j^N exp(a_j)}$$
其中$a_k = \ln p((x|C_k)p(C_k))$
这被成为归一化指数或者又叫softmax函数

### 4.2.1 连续输入

如果类的条件概率密度满足高斯分布:

$$p(x|C_k) = \frac{1}{(2\pi)^{D/2}}\frac{1}{(\Sigma)^{1/2}}exp\big \{ -\frac1 2(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \big\}$$

`这里假定了每一个Class的方差矩阵`$\Sigma$`都相同，不然`$x^2$`不会被消去，结果会是一个二次方程而不是线性方程`

带入到2分类问题sigmoid函数中可以得到

$$p(C_1|x) = \sigma(w^Tx+w_0)$$
其中有$w=\Sigma^{-1}(\mu_1 - \mu_2)$ , $w_0=-\frac 1 2 \mu_1^T\Sigma^{-1}\mu_1 + \frac 1 2 \mu_2^T\Sigma^{-1}\mu_2 + \ln \frac {p(C_1)} {p(C_2)}$

可以得到K类的线性方程：
$$a_k(x) = w_k^Tx + w_{k0} $$
$$ w_k = \Sigma^{-1}\mu_k $$
$$ w_{k0} = -\frac12\mu_k^T\Sigma^{-1}\mu_k + ln p(C_k)$$
$a_k$也是线性函数，这是因为$\Sigma$相同，二次项被消去。最后分界面两个类目概率相等的位置，由线性函数定义。而如果不同类别协方差不同，就会引入二次判别函数。

### 4.2.2 最大似然解
一旦确定了条件概率密度的参数化形式，就可以使用最大似然方法在具体的数据集上求出$p(x|C_k)$的值。考虑两类情况，每个类别都有一个高斯密度函数，而且协方差相同，两个类目的先验概率分别为$\pi$和$1 - \pi$

$$p(x_n, C_1) = \pi N(x_n|\mu_1, \Sigma)$$
$$p(x_n, C_2) = (1 - \pi) N(x_n|\mu_2, \Sigma)$$
求最大似然可以得到$\pi = \frac {N_1}{N_1 + N_2}$，这个结果也可以很容易推广到多类。
求$\mu$的最大似然：整理可得
$$\mu_1 = \frac 1 {N_1} \sum_{n=1} N t_nx_1$$
$$\mu_2 = \frac 1 {N_2} \sum_{n=1} N (1-t_n)x_1$$
方差满足
$$S=\frac {N_1} N S_1 + \frac {N_2} N S_2$$
$$S_1=\frac 1 {N_1} \sum_{n\epsilon C_1}(x_n-\mu_1)(x_n -\mu_1)^T$$
$$S_2=\frac 1 {N_2} \sum_{n\epsilon C_2}(x_n-\mu_2)(x_n -\mu_2)^T$$

### 4.2.3 离散特征
考虑离散二元特征，有D个输入，做朴素贝叶斯近似可以得到:
$$p(x|C_k)=\prod_{i=1}^D\mu_{k_i}^{x_i}(1-mu_{k_i})^{1-x_i}$$
带入公式可以得到
$$a_k(x) = \sum_{i=1}^D\{x_i \ln\mu_{k_i} + (1-x_i)\ln(1-\mu_i)  \} + \ln p(C_k)$$
这也是一个线性函数，可以利用logistic sigmoid 处理。

### 4.2.4 指数族分布
无论高斯分布特征还是离散特征后验概率都可以用线性函数 + sigmoid / softmax 激活函数给出。通过假定条件概率密度$p(x|C_k)$是指数分布成员可以得到一般性结论。
$$p(x|\lambda_k) = h(x)g(\lambda_k)exp\{\lambda_k^T\mu(x)\}$$
当$\mu_x = x$ 时可以得到
$$a_k(x) = \frac 1 s \lambda_x^T x + \ln g(\lambda_k) + \ln p(C_k)$$

## 4.3 概率判别模型
上一节介绍了从概率触发一大类条件概率密度可以通过线性函数加激活函数解决。本节介绍直接用线性函数加激活函数建模，然后直接用最大似然确定其权重。这是一种判别方法。

### 4.3.1 固定基方程
使用非线性函数$\phi(x)$对固定基进行线性变换后，上面所有内容依然适用。实际场景中的不同类别概率密度往往会有混叠，固定非线性基函数变换可以让混叠建模更简单，不过固定非线性基函数有它先天的局限性，后面章节中非固定基函数可以克服这些问题。

### 4.3.2 逻辑回归
对二分类问题假设类别的后验概率可以写成logistic sigmoid 形式
$$p(C_k| x) = y(x) = \sigma(w^Tx)$$
对高斯生成模型，$M$个参数需要$M$个均值和$M^2$个协方差。而逻辑回归只要M个参数。对高斯生成模型，迭代一次就能到最小值，而对于逻辑回归每次迭代迭代量都会改变，只能逼近最小值。

用最大似然求模型参数两分类概率方程为 
$$p(t|w) = \prod_{n=1}^N y_n^{t_n}{1-y_n}^{1-t_n}$$
误差方程为**交叉熵(cross entropy)**损失函数：
$$E(w) = -lnp(t|w) = \sum_{n=1}^N \big\{t_n*y_n + (1-t_n)*(1-y_n)\big\}$$
将 $y=\simga(w^Tx)$ 带入可以得到:

$$\triangledown E(w) = \sum_{n=1}^N(y_n - t_n) \phi_n $$

其中$y_n$为预测值，这个函数可以通过迭代方法求解最小值。最大似然方法对于线性可分的问题会产生过拟合的现象。还有最大似然法无法区分某个解优于另一个解。通过正则化可以避免这些奇异性。

### 4.3.3 迭代权重最小平方
使用牛顿迭代，函数形式为：
$$w_{new} = w_{old} - H^{-1}\triangledown E(w)$$
带入可以得到
$$ \triangledown E(w) = \phi^T\phi w - \phi^Tt$$
$$H = \triangledown \triangledown E(w) = \phi^T\phi $$

这个方法被称为迭代权重加权最小平方。（IRLS）

### 4.3.4 多类罗辑回归

多分类误差方程为多分类交叉熵函数：
$$E(w_1,w_2...w_k) = -lnp(T|w_1,w_2...w_k) =  \sum_{k=1}^K  \sum_{n=1}^N y_{nk}^{t_{nk}}$$
可以使用IRLS方法求其最小值

### 4.3.5 probit回归
不是所有类的条件概率都有如logistic 回归一样简单的形式。考虑如下模型，

$$f(a) = 1 , a >= \theta $$
$$f(a) = 0 , a < \theta $$

其中$\theta$由分布$p(\theta)$确定如果$p(\theta)$为一个均值为0，方差为1的高斯分布。则称为**probit**回归。

### 4.3.6 标准链接函数(canonical link functions)
对于指数簇中误差函数，负对数似然函数。如果我们对数据点n对误差函数的贡献关于参数向量w求导数，那么导数的形式为$y_n - t_n$ 与特征向量$\phi_n$的乘积，其中$y_n = \phi_n x$。可以证明如果假设目标变量的条件分布来自于指数族分布， 对应的激活函数选为标准链接函数(canonical link function)，那么这个结果是一个一般的结果。

$$\triangledown E(w) = \frac 1 s \sum({y_n - t_n})\phi_n$$
对高斯$s=\beta^{-1}$ 对logistic模型 $s=1$

## 4.4 拉普拉斯近似
本节介绍了一种广泛使用框架 -- 拉普拉斯近似。 目标是找到定义在一组连续变量密度函数$p(x)$的高斯近似。方法为首先找到$p(z)$的众数$z_0$，然后计算这个位置上的Hessian矩阵H，这样就有近似$q(x) = N(x|z_0, H)$。

### 4.4.1 模型比较和BIC
考虑对数据集D从模型{$M_i$}中做选择，将拉普拉斯近似求最大概率的导数，可以得到:
 $$\ln p(D) \simeq \ln p(D|\theta_{MAP}) + \ln p(D|\theta_{MAP}) + \frac M 2ln(2\pi) - \frac 1 2 ln |A| $$
假设参数高斯先验分布比较宽，$\ln(D|\theta_{MAP})$为常数，可以更进一步近似。
$$\ln p(D) = \ln p(D|\theta_{MAP}) + \frac 1 2 M \ln N$$
其中 M 为参数数量 N 为数据点数。这个被称为 **贝叶斯信息准则Bayesian information criterion (BIC)**

## 4.5 贝叶斯逻辑回归
本节考虑logistic 回归的贝叶斯观点。传统方法无法贝叶斯化，这里采用拉普拉斯近似处理。

### 4.5.1 拉普拉斯近似
使用拉普拉斯近似可以得到：后验概率为
$$q(w) = N(w|w_{MAP}, S_N)$$

### 4.5.2 预测分布
给定t预测$C_1$的概率分布：得到
$$p(c_1|t) = \int \sigma(a)p(a)da = \int\sigma(a)N(a|\mu_a,\sigma_a^2)da$$
使用上文提到的**probit regression**可以得到

$$\int\sigma(a)N(a|\mu,\sigma^2)da \simeq \sigma(k(\sigma^2)\mu)$$ 
$$k(\sigma^2) = (1+\frac{\pi\sigma^2}8)^\frac 1 2$$
