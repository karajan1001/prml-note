# 第四章 线性模型分类
分类问题是把输入映射到离散的类别中。每个输入被分到唯一的一个类别中，输入空间按照所分类别被划分为不同的**决策区域(decision region)**,区域之间的边界叫**决策边界（decision boundaries）**。而线性分类模型是指的，分类边界是输入的线性函数，也就是说是D维输入空间中由D-1个变量决定的一个超平面。在二分类问题中一般使用0，1来表示分类。而多分类问题中，更多的是使用1-of-K coding表示。对于线性回归问题。有
$$y(x) = w^Tx + w_0$$
预测的结果为一个实数。对于线性回归问题，我们希望预测的结果是一个离散标签，或者更一般的是（0，1）之间的一个概率值，此时我们可以使用推广
$$y(x) = f(w^Tx + w_0)$$
这里的$f(x)$被称为**激活函数(activation function)**它的反函数在统计文献中被成为**链接函数(link function)**,决策面对应$y(x)$等于常数。带有激活函数的模型被成为推广线性模型，但是它们其实是非线性的，因为激活函数往往不是线性的。
## 4.1 判别函数
判别函数是一个函数，给定一个输入x将其分配到K个类目中的一个的函数。本节注意力集中于线性判别函数或者是决策面是超平面的判别函数。

### 4.1.1 二分类
判别函数中最简单的形式是输入向量的线性函数。

$$y(x) = w_T x + w_0$$
其中，$w_T$被称为**权向量(weight vector)**，而$w_0$叫**偏置(bias)**，它的相反数叫做**阈值(threshold)**。对于一个输入当$y(x)>0$它属于$C_1$ 反之属于$C_2$。决策分界面由$y(x)=0$确定。因为对于决策平面上任意点$x_A,x_B$都有$w_T(x_A-x_B)=0$所以对于决策平面上的任意向量都与$w_T$正交。任意一点到超平面的距离为$\frac{y(x)}{||w_T||}$对于原点，到超平面的距离为$\frac{w_0}{||w_T||}$。引入一个额外输入$x_0=1$则公式可以化简为
$$y(x) = \hat w_T \hat x$$

### 4.1.2 多分类

对多分类问题如果从二分类扩展，one-versus-one 或者 one-versus-the-rest 方法都会导致某些区域要么类别无法确定，要么分类冲突，一个可行的方法是N个方程。
$$y_k(x)=w_k^Tx+w_{k0}$$
对任意一个点如果第K个方程值最大，则它属于分类K。
可以证明，这个方程组形成的分类区域是联通而且凸的。

### 4.1.3 最小二乘
如果对分类问题使用类似回归问题的最小二乘法可以得到类似的解。但是这种解法有两个问题：
1. 输出$y_k(x)$不在[0, 1]区间内，也不满足所有类目的概率之和为1。
2. 第二，缺少鲁棒性，对于过于正确的点会引起分界面明显的错误变化。
最小二乘法的问题在于，分类问题预测值为0，1明显不满足高斯分布。

### 4.1.4 Fisher的线性判别式

将数据投影到一维平面，fisher的判别式方程寻找一个投影方向，最大化类平均值的间距的同时，最小化类方差。具体方程为
$$J(w) = \frac{(m_2-m_1)^2}{s_1^2+s_2^2}$$
其中$m_1,m_2$ 为两个类的均值向量
$$m_1= \frac 1 {N_1} \sum_{n \epsilon C_1} x_n;
m_2= \frac 1 {N_2} \sum_{n \epsilon C_2} x_n$$
$$s_1= \sum_{n \epsilon C_1}(x_n - m_1)^2;s_2= \sum_{n \epsilon C_2}(x_n - m_1)^2$$

这个式子可以化为
$$J(w) = \frac {w_TS_Bw} {w_TS_Ww}$$
其中
$$S_W = \sum_{n\epsilon C_1} (x_n-m_1)(x_n - m_1)^T + \sum_{n\epsilon C_2} (x_n-m_2)(x_n - m_2)^T $$
$$S_B = (m_2-m_1)(m_2-m_1)^T$$

其解为$w \propto S_W^{-1}(m_2-m_1)$

### 4.1.5 Fisher是最小二乘的特例
如果设定两个类目的目标值为
$$t_{c_1} = \frac N{N_1} ; t_{c_2} = -\frac N{N_2} $$ 
对这两个目标值使用最小二乘则可以推导出Fisher判别式。其中N是总样本，$N_1$和$N_2$分别为1，2类别的样本量。

### 4.1.6 多分类的Fisher判别函数
将Fisher判别函数推广到N分类问题。可以得到
$$J(w) = Tr\{(W^TS_WW)^{-1}(W^TS_BW)\}$$

### 4.1.7 感知器算法
先将输入x转化为一组坐标基$\phi(x)$ 然后利用激活函数将线性函数$w^T\phi(x)$

$$y(x) = f(w^T \phi(x))$$

其中f是非线性激活函数。满足$f(x) = 1$当$x>0$时$f(x) = -1$当$x<0$时。 但是这个函数不可导，所以要引入额外的误差函数
$$E_p(w) = - \sum_{n\epsilon M}w^T\phi_n t_n$$ 
其中$t$对不同类别分别等于正负1。对误差使用梯度随机下降。
读线性可分数据，感知器有限步必然会收敛，但是如果数据不可分，则不会收敛。而且感知器对$K>2$的情况没法使用

## 4.2 概率生成模型
利用贝叶斯定力有
$$p(C_1|x) = \frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1) + p(x|C_2)p(C_2)}$$
假设
$$ a = ln\frac{p(x|c_1)p(c_1)}{p(x|c_2)p(c_2)}$$
则可以得到**logistic sigmoid** 方程:
$$ p(C_1|x) = \sigma(a)  = \frac1{1+exp(-a)} $$ 
sigmoid函数，意义为s形，或者叫挤压函数。
$$\sigma(-a) = 1 - \sigma(a)$$
而它的反函数
$$a=\ln(\frac{\sigma}{1-\sigma})$$
被称为logit函数表示两个类概率比值的对数。
对于多分类问题有:
$$p(C_k|x) = \frac{p(x|C_k)p(C_k)}{\sum_j^N p(x|C_j)p(C_j)}$$
$$p(C_k|x) = \frac{exp(a_k))}{\sum_j^N exp(a_j)}$$
其中$a_k = \ln p((x|C_k)p(C_k))$
这被成为归一化指数或者又叫softmax函数

### 4.2.1 连续输入

如果类的条件概率密度满足高斯分布:

$$p(x|C_k) = \frac{1}{(2\pi)^{D/2}}\frac{1}{(\Sigma)^{1/2}}exp\big \{ -\frac1 2(x-\mu_k)^T\Sigma^{-1}(x-\mu_k) \big\}$$

`这里假定了每一个Class的方差矩阵`$\Sigma$`都相同，不然`$x^2$`不会被消去，结果会是一个二次方程而不是线性方程`

带入到2分类问题sigmoid函数中可以得到

$$p(C_1|x) = \sigma(w^Tx+w_0)$$
其中有$w=\Sigma^{-1}(\mu_1 - \mu_2)$ , $w_0=-\frac 1 2 \mu_1^T\Sigma^{-1}\mu_1 + \frac 1 2 \mu_2^T\Sigma^{-1}\mu_2 + \ln \frac {p(C_1)} {p(C_2)}$

可以得到K类的线性方程：
$$a_k(x) = w_k^Tx + w_{k0} $$
$$ w_k = \Sigma^{-1}\mu_k $$
$$ w_{k0} = -\frac12\mu_k^T\Sigma^{-1}\mu_k + ln p(C_k)$$
$a_k$也是线性函数，这是因为$\Sigma$相同，二次项被消去。最后分界面两个类目概率相等的位置，由线性函数定义。而如果不同类别协方差不同，就会引入二次判别函数。

### 4.2.2 最大似然解
一旦确定了条件概率密度的参数化形式，就可以使用最大似然方法在具体的数据集上求出$p(x|C_k)$的值。考虑两类情况，每个类别都有一个高斯密度函数，而且协方差相同，两个类目的先验概率分别为$\pi$和$1 - \pi$

$$p(x_n, C_1) = \pi N(x_n|\mu_1, \Sigma)$$
$$p(x_n, C_2) = (1 - \pi) N(x_n|\mu_2, \Sigma)$$
求最大似然可以得到$\pi = \frac {N_1}{N_1 + N_2}$，这个结果也可以很容易推广到多类。
求$\mu$的最大似然：整理可得
$$\mu_1 = \frac 1 {N_1} \sum_{n=1} N t_nx_1$$
$$\mu_2 = \frac 1 {N_2} \sum_{n=1} N (1-t_n)x_1$$
方差满足


### 4.2.3 离散特征

### 4.2.4 指数族分布

## 4.3 概率判别模型
概率判别模型比概率生成模型简单，分类时准确率更高。

### 4.3.1 固定基方程
非线性固定基可以使得某些基下不可分的方程线性可分。但是某些情况时不同类型的点混在一起，就算是非线性变换也不能把它们分开。

### 4.3.2 逻辑回归
对高斯生成模型，$M$个参数需要$M$个均值和$M^2$个协方差。而逻辑回归只要M个参数。对高斯生成模型，迭代一次就能到最小值，而对于逻辑回归每次迭代迭代量都会改变，只能逼近最小值。

两分类概率方程为 ：$$p(t|w) = \prod_{n=1}^N y_n^{t_n}{1-y_n}^{1-t_n}$$
误差方程为：$$E(w) = -lnp(t|w) = \sum_{n=1}^N \big\{t_n*y_n + (1-t_n)*(1-y_n)\big\}$$
多分类概率方程为：
多分类误差方程为**交叉熵方程(cross entropy)**：$$E(w_1,w_2...w_k) = -lnp(T|w_1,w_2...w_k) =  \sum_{k=1}^K  \sum_{n=1}^N y_{nk}^{t_{nk}}$$

**probit regression**：很像逻辑回归，由一个均值为0，方差为1的高斯分布积分得来。

### 4.3.3 迭代权重最小平方

### 4.3.4 多类罗辑回归

### 4.3.5 probit回归

### 4.3.6 标准链接函数(canonical link functions)
高斯分布的log似然方程对第n个点的导数同样可以从逻辑回归 交叉熵方程对第n个点求导得出，更一般只要是指数家族都可以

## 4.4 拉普拉斯近似
未知分布，用高斯分布去近似，均值为最大值位置，精度为最大值位置的二阶导数。

### 4.4.1 模型比较和BIC
 *Bayesian information criterion (BIC)*：将拉普拉斯近似求最大概率的导数，可以得到$$lnp(D) \simeq lnp(D|\theta_{MAP}) + lnp(D|\theta_{MAP}) + \frac M 2ln(2\pi) - \frac 1 2 ln |A| $$

## 4.5 贝叶斯逻辑回归

### 4.5.1 拉普拉斯近似
使用拉普拉斯近似可以得到：后验概率为$$q(w) = N(w|w_{MAP}, S_N)$$

### 4.5.2 预测分布
给定t预测$C_1$的概率分布：得到$$p(c_1|t) = \int \sigma(a)p(a)da = \int\sigma(a)N(a|\mu_a,\sigma_a^2)da$$
使用上文提到的**probit regression**可以得到

$$\int\sigma(a)N(a|\mu,\sigma^2)da \simeq \sigma(k(\sigma^2)\mu) \\
k(\sigma^2) = (1+\frac{\pi\sigma^2}8)^\frac 1 2$$
